import csv
import os
import random
import string
import time

import numpy as np
import pandas as pd
import torch
import transformers
from huggingface_hub import login
from sklearn.model_selection import train_test_split
from tqdm import tqdm

from src.utils import format_time, load_balanced_dataset, print_metrics
from src.viz import compute_label_balance

torch.cuda.empty_cache()
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

class Llama3Feedback:
    def __init__(self, model_id="meta-llama/Meta-Llama-3-8B-Instruct"):
        """
        Initialize the Llama3Model with the specified model ID.
        The model uses the transformers pipeline for text generation with instruction-based capabilities.
        """
        self.pipeline = transformers.pipeline(
            "text-generation",
            model=model_id,
            model_kwargs={"torch_dtype": torch.bfloat16},
            device="cuda"
        )

    def is_text_ai_generated(self, text, max_length, prompt, do_sample=False, temperature=None, top_p=None):
        """
        Asks the model if the provided text is generated by an AI, requiring a simple 'yes' or 'no' answer.
        Uses the instruction-based capabilities of the Llama Instruct model.
        """
        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": text[:max_length]}
        ]

        prompt = self.pipeline.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        terminators = [
            self.pipeline.tokenizer.eos_token_id,
            self.pipeline.tokenizer.convert_tokens_to_ids("<|eot_id|>")
        ]

        outputs = self.pipeline(
            prompt,
            max_new_tokens=1,
            eos_token_id=terminators,
            do_sample=do_sample,
            temperature=temperature,
            top_p=top_p,
            pad_token_id=self.pipeline.tokenizer.eos_token_id
        )

        response = outputs[0]["generated_text"].strip()
        last_word = response.split()[-1].strip().lower()
        return 1 if "yes" in last_word else 0 if "no" in last_word else None
    
    def process_texts_sequentially(self, texts, max_length, prompt, do_sample, temperature, top_p):
        """
        Applies the is_ai_generated function to each text in the given list, displaying a progress bar.
        """
        results = []
        for text in tqdm(texts, desc="Processing texts"):
            results.append(self.is_text_ai_generated(text, max_length, prompt, do_sample, temperature, top_p)) 
        return results
    
    def find_max_length(self, prompt, do_sample=False, temperature=None, top_p=None):
        """
        Uses binary search to find the optimal text length to avoid CUDA memory errors.
        Rounds to the nearest 1,000 to refine the process. Generates random text for testing.
        """
        def causes_cuda_error(length):
            text = ''.join(random.choices(string.ascii_letters + string.digits + ' ', k=length))
            try:
                self.is_text_ai_generated(text, length, prompt, do_sample, temperature, top_p)
                return False
            except Exception:
                return True
        
        initial_high = 100000
        high = initial_high
        while not causes_cuda_error(high):
            high *= 2
            print(f"Trying with high = {high}")
        
        print(f"Found `high` value that causes CUDA error: {high}")
        
        low = initial_high if high != initial_high else 10000
        
        best_length = low
        iters = 0
        while low <= high:
            mid = (low + high) // 2
            mid = 1000 * round(mid / 1000)  # Round to the nearest 1,000
            if causes_cuda_error(mid):
                high = mid - 1000
            else:
                best_length = mid
                low = mid + 1000
            iters += 1
        
        print(f"Found max length without raising CUDA error in {iters} iteration(s): {best_length}")
        return best_length

    def find_best_hyperparameters(self, prompts, texts, labels, output_csv="data/prompt_optimizations_no_none.csv"):
        do_sample_options = [True, False]
        temperature_options = [0.5, 0.6, 0.7, 0.8]
        top_p_options = [0.6, 0.7, 0.8, 0.9]

        longest_prompt = max(prompts, key=len)
        optimal_max_length = self.find_max_length(longest_prompt)
        
        with open(output_csv, 'w', newline='') as file:
            writer = csv.writer(file)
            writer.writerow(['Prompt', 'do_sample', 'temperature', 'top_p', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'Confusion Matrix', 'None Count', 'Label Balance'])

            for prompt in prompts:
                for do_sample in do_sample_options:
                    temp_options = temperature_options if do_sample else [None]
                    p_options = top_p_options if do_sample else [None]
                    for temperature in temp_options:
                        for top_p in p_options:
                            print(f"Testing config: Prompt='{prompt}', do_sample={do_sample}, temperature={temperature}, top_p={top_p}")
                            predictions = self.process_texts_sequentially(texts, optimal_max_length, prompt, do_sample, temperature, top_p)
                            
                            valid_indices = [i for i, pred in enumerate(predictions) if pred is not None]
                            valid_predictions = [predictions[i] for i in valid_indices]
                            valid_labels = [labels[i] for i in valid_indices]
                            
                            # Calculate metrics
                            accuracy, precision, recall, f1, conf_matrix = print_metrics(valid_labels, valid_predictions, verbose=True)
                            label_balance = compute_label_balance(conf_matrix)
                            none_count = len(predictions) - len(valid_predictions)
                            
                            writer.writerow([prompt, do_sample, temperature, top_p, accuracy, precision, recall, f1, str(conf_matrix), none_count, label_balance])
                            print(f"Metrics: {accuracy, precision, recall, f1, conf_matrix}, None count: {none_count}, Label balance: {label_balance}")

    def write_results_to_csv(self, results):
        fieldnames = [
            "task_description", "output_instruction", "yes_no_order",
            "unique_prompt_id", "text_id", "text_label", "prediction"
        ]
        with open('data/prompt_experiment_results.csv', 'w', newline='') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            for result in results:
                writer.writerow(result)

    def run_prompt_experiments(self, experiments, texts, text_ids, text_labels):
        longest_prompt = max((td + oi for td in experiments["Task Description"].values()
                                      for oi in list(experiments["Output Instruction"]["Yes/No"].values()) +
                                                list(experiments["Output Instruction"]["No/Yes"].values())), key=len)
        
        optimal_max_length = self.find_max_length(longest_prompt)
        results = []
        total_combinations = sum(len(output_instructions) for output_instructions in experiments["Output Instruction"].values()) * len(experiments["Task Description"])

        with tqdm(total=total_combinations, desc="Running Prompt Experiments") as pbar:
            for td_key, task_desc in experiments["Task Description"].items():
                for oi_key, output_instructions in experiments["Output Instruction"].items():
                    for oi_desc_key, output_desc in output_instructions.items():
                        prompt = f"{task_desc} {output_desc}"
                        predictions = self.process_texts_sequentially(texts, optimal_max_length, prompt, do_sample=False, temperature=None, top_p=None)
                        for text, text_id, label, prediction in zip(texts, text_ids, text_labels, predictions):
                            result = {
                                "task_description": task_desc,
                                "output_instruction": output_desc,
                                "yes_no_order": oi_key,
                                "unique_prompt_id": f"{td_key[0].upper()}{oi_key[0].upper()}{oi_desc_key[0].upper()}",
                                "text_id": text_id,
                                "text_label": label,
                                "prediction": prediction if prediction is not None else np.nan
                            }
                            results.append(result)
                        pbar.update(1)
        
        self.write_results_to_csv(results)

    def rewrite_text(self, text, max_length, prompt, do_sample=True, temperature=0.7, top_p=0.9):
        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": text[:max_length]}
        ]

        prompt = self.pipeline.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        terminators = [
            self.pipeline.tokenizer.eos_token_id,
            self.pipeline.tokenizer.convert_tokens_to_ids("<|eot_id|>")
        ]

        outputs = self.pipeline(
            prompt,
            max_new_tokens=1024,
            eos_token_id=terminators,
            do_sample=do_sample,
            temperature=temperature,
            top_p=top_p,
            pad_token_id=self.pipeline.tokenizer.eos_token_id
        )

        response = outputs[0]["generated_text"]
        return response
    
    def rewrite_texts(self, prompt, texts):
        max_length = self.find_max_length(prompt)
        rewritten_texts = []
        for text in tqdm(texts, desc="Rewriting texts"):
            rewritten_texts.append(self.rewrite_text(text, max_length, prompt))
        return rewritten_texts
    
def write_to_csv(name, texts, ids):
    output_file = f'data/rephrased_texts_{name}.csv'
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(['id', 'text'])
        for id_, text in zip(ids, texts):
            writer.writerow([id_, text])

def get_llama3_evasive_feedback():
    start_time = time.time()

    print("CUDA available:", torch.cuda.is_available())
    print("CUDA version:", torch.version.cuda)
    print("Current CUDA device:", torch.cuda.current_device())
    
    llama_model = Llama3Feedback()
    
    evasive_datasets = [
        'data/evasive_texts/control.csv', 
        'data/evasive_texts/basic.csv', 
        'data/evasive_texts/advanced.csv'
    ]
    
    for dataset in evasive_datasets:
        df = pd.read_csv(dataset)
        texts = df['rewritten_text'].tolist()
        ids = df['id'].tolist()
        labels = [1] * len(texts)

        prompt = "Considering typical AI text traits such as repetitive phrasing, overly formal language, and minimal errors, determine if the following text is AI-generated. Respond with 'no' if the text is human-written, 'yes' if the text is AI-generated."

        optimal_max_length = llama_model.find_max_length(prompt)
        sequential_predictions = llama_model.process_texts_sequentially(texts, optimal_max_length, prompt, do_sample=False, temperature=None, top_p=None)

        dataset_name = os.path.basename(dataset).replace('.csv', '')
        output_dir = f'data/evasive_texts/feature/{dataset_name}'
        os.makedirs(output_dir, exist_ok=True)
        feedback_file_name = os.path.join(output_dir, f'{dataset_name}_ai_feedback.csv')

        with open(feedback_file_name, 'w', newline='') as file:
            writer = csv.writer(file)
            writer.writerow(['id', 'pred', 'label'])
            for id, pred, label in tqdm(zip(ids, sequential_predictions, labels), total=len(ids), desc="Writing to CSV"):
                writer.writerow([id, pred, label])

        none_count = sum(1 for pred in sequential_predictions if pred is None)
        print("Number of None values in predictions:", none_count)
        filtered_predictions = [0 if pred is None else pred for pred in sequential_predictions]

        print_metrics(labels, filtered_predictions)

    print(f"Total runtime: {format_time(time.time() - start_time)}")

def get_llama3_texts():
    start_time = time.time()

    print("CUDA available:", torch.cuda.is_available())
    print("CUDA version:", torch.version.cuda)
    print("Current CUDA device:", torch.cuda.current_device())

    llama_model = Llama3Feedback()

    dataset = load_balanced_dataset(subset_size=None)

    texts = [entry['text'] for entry in dataset]
    ids = [entry['id'] for entry in dataset]
    labels = [1 if entry['source'] == 'ai' else 0 for entry in dataset]

    x_train, x_test, ids_train, ids_test, labels_train, labels_test = train_test_split(texts, ids, labels, test_size=0.1, random_state=42)

    x_test_ai = [text for text, label in zip(x_test, labels_test) if label == 1][:5000]
    ids_test_ai = [id_ for id_, label in zip(ids_test, labels_test) if label == 1][:5000]

    prompts = {
        "Task Description": {
            "Control": "Rewrite the following text in your own style and tone.",
            "Basic": "Rewrite the following text so it won't be detected by AI detection tools.",
            "Advanced": "Considering typical AI text traits such as repetitive phrasing, overly formal language, and minimal errors, rewrite the following text so it won't be detected by AI detection tools."
        },
        "Output Instruction": "Respond with only the rewritten text."
    }

    combined_prompts = {
        name: f"{task_desc} {prompts['Output Instruction']}"
        for name, task_desc in prompts["Task Description"].items()
    }
    
    for name, prompt in combined_prompts.items():
        print(f"Rewriting texts with '{name}' prompt...")
        rewritten_texts = llama_model.rewrite_texts(prompt, x_test_ai)
        write_to_csv(name, rewritten_texts, ids_test_ai)
    
    print(f"Total runtime: {format_time(time.time() - start_time)}")

def get_llama3_feedback():
    start_time = time.time()

    print("CUDA available:", torch.cuda.is_available())
    print("CUDA version:", torch.version.cuda)
    print("Current CUDA device:", torch.cuda.current_device())
    
    llama_model = Llama3Feedback()

    dataset = load_balanced_dataset()

    texts = [entry['text'] for entry in dataset]
    ids = [entry['id'] for entry in dataset]
    labels = [1 if entry['source'] == 'ai' else 0 for entry in dataset]

    prompt = "Considering typical AI text traits such as repetitive phrasing, overly formal language, and minimal errors, determine if the following text is AI-generated. Respond with 'no' if the text is human-written, 'yes' if the text is AI-generated."

    # experiments = {
    #     "Task Description": {
    #         "Simple": "Determine if the following text is AI-generated.",
    #         "Moderate": "Considering typical AI text traits, determine if the following text is AI-generated.",
    #         "Detailed": "Considering typical AI text traits such as repetitive phrasing, overly formal language, and minimal errors, determine if the following text is AI-generated."
    #     },
    #     "Output Instruction": {
    #         "Yes/No": {
    #             "Simple": "Respond with 'yes' or 'no'.",
    #             "Moderate": "Respond with 'yes' if the text is AI-generated, 'no' otherwise.",
    #             "Detailed": "Respond with 'yes' if the text is AI-generated, 'no' if the text is human-written."
    #         },
    #         "No/Yes": {
    #             "Simple": "Respond with 'no' or 'yes'.",
    #             "Moderate": "Respond with 'no' if the text is not AI-generated, 'yes' otherwise.",
    #             "Detailed": "Respond with 'no' if the text is human-written, 'yes' if the text is AI-generated."
    #         }
    #     }
    # }
    
    # llama_model.run_prompt_experiments(experiments, texts, ids, labels)

    # best_prompts = [
    #     "Determine if this text was generated by an AI. Say 'yes' if it was, 'no' if it wasn't.",
    #     "Review the following text and indicate with a 'yes' if it is AI-generated, or 'no' if it is human-written.",
    #     "Analyze and identify if the following text was generated by an AI. Respond with 'yes' if it is AI-generated, otherwise, respond with 'no'."
    # ]
    # llama_model.find_best_hyperparameters(best_prompts, texts, labels)

    optimal_max_length = llama_model.find_max_length(prompt)
    sequential_predictions = llama_model.process_texts_sequentially(texts, optimal_max_length, prompt, do_sample=False, temperature=None, top_p=None)

    with open('data/llama_feedback_results_dnd.csv', 'w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(['id', 'pred', 'label'])
        for id, pred, label in tqdm(zip(ids, sequential_predictions, labels), total=len(ids), desc="Writing to CSV"):
            writer.writerow([id, pred, label])

    none_count = sum(1 for pred in sequential_predictions if pred is None)
    print("Number of None values in predictions:", none_count)
    filtered_predictions = [0 if pred is None else pred for pred in sequential_predictions]

    print_metrics(labels, filtered_predictions)

    print(f"Total runtime: {format_time(time.time() - start_time)}")

if __name__ == "__main__":
    get_llama3_texts()